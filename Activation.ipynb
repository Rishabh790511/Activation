{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23a0476-c331-4aca-88dc-7ddbd5c06d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. An activation function in the context of artificial neural networks is a mathematical operation applied to the output of each neuron in a neural network layer. It determines whether the neuron should be activated or not, based on whether the neuron's input exceeds a certain threshold. Activation functions introduce non-linearity to the network, allowing it to learn complex patterns in the data.\n",
    "\n",
    "Q2. Some common types of activation functions used in neural networks include:\n",
    "\n",
    "Sigmoid\n",
    "Hyperbolic tangent (tanh)\n",
    "Rectified Linear Unit (ReLU)\n",
    "Leaky ReLU\n",
    "Softmax\n",
    "Q3. Activation functions affect the training process and performance of a neural network in several ways, including:\n",
    "\n",
    "Introducing non-linearity, enabling the network to learn complex relationships in the data.\n",
    "Influencing the speed and stability of training by affecting the gradients during backpropagation.\n",
    "Affecting the range of values that neurons can output, which can impact convergence and gradient propagation.\n",
    "Q4. The sigmoid activation function, also known as the logistic function, maps the input values to a range between 0 and 1. It is defined as:\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "1\n",
    "1\n",
    "+\n",
    "�\n",
    "−\n",
    "�\n",
    "σ(x)= \n",
    "1+e \n",
    "−x\n",
    " \n",
    "1\n",
    "​\n",
    " \n",
    "Advantages:\n",
    "\n",
    "It squashes input values to a range suitable for binary classification tasks.\n",
    "It produces smooth gradients, making it well-suited for gradient-based optimization algorithms like gradient descent.\n",
    "Disadvantages:\n",
    "Sigmoids can suffer from the vanishing gradient problem, where gradients become extremely small, leading to slow convergence, especially in deep networks.\n",
    "They are not zero-centered, which can hinder the convergence of the gradient descent algorithm.\n",
    "Q5. The Rectified Linear Unit (ReLU) activation function is defined as \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "max\n",
    "⁡\n",
    "(\n",
    "0\n",
    ",\n",
    "�\n",
    ")\n",
    "f(x)=max(0,x). Unlike the sigmoid function, ReLU is linear for positive inputs and zero for negative inputs. This function is computationally efficient and has become popular in deep learning due to its ability to mitigate the vanishing gradient problem and accelerate convergence.\n",
    "\n",
    "Q6. Benefits of using the ReLU activation function over the sigmoid function include:\n",
    "\n",
    "ReLUs address the vanishing gradient problem by avoiding saturation for positive inputs, leading to faster convergence during training.\n",
    "They are computationally efficient to compute compared to sigmoid and tanh functions.\n",
    "ReLUs provide sparse activations, which can aid in regularization and prevent overfitting.\n",
    "Q7. \"Leaky ReLU\" is a variation of the ReLU activation function that addresses the vanishing gradient problem by allowing a small, non-zero gradient for negative inputs. It is defined as:\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "{\n",
    "�\n",
    ",\n",
    "if \n",
    "�\n",
    ">\n",
    "0\n",
    "�\n",
    "�\n",
    ",\n",
    "otherwise\n",
    "f(x)={ \n",
    "x,\n",
    "ax,\n",
    "​\n",
    "  \n",
    "if x>0\n",
    "otherwise\n",
    "​\n",
    " \n",
    "where \n",
    "�\n",
    "a is a small constant, typically around 0.01. By allowing a small gradient for negative inputs, leaky ReLU prevents the neuron from being completely inactive, ensuring that information can still flow through the network during training.\n",
    "\n",
    "Q8. The softmax activation function is commonly used in the output layer of a neural network for multi-class classification tasks. It normalizes the output of a layer into a probability distribution over multiple classes, ensuring that the output values sum up to 1. Softmax is useful when the neural network needs to provide probabilities for multiple mutually exclusive classes.\n",
    "\n",
    "Q9. The hyperbolic tangent (tanh) activation function is similar to the sigmoid function but maps input values to a range between -1 and 1. It is defined as:\n",
    "tanh\n",
    "⁡\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "−\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "−\n",
    "�\n",
    "tanh(x)= \n",
    "e \n",
    "x\n",
    " +e \n",
    "−x\n",
    " \n",
    "e \n",
    "x\n",
    " −e \n",
    "−x\n",
    " \n",
    "​\n",
    " \n",
    "Compared to the sigmoid function, tanh is zero-centered, which helps in the optimization process. However, it still suffers from the vanishing gradient problem for deep networks. Tanh is often used in hidden layers of neural networks for tasks where the output range needs to be centered around zero."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
